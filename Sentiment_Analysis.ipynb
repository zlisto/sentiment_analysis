{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYBXLV8IolqF"
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "In this notebook we will learn how to measure sentiment in text. \n",
    "\n",
    "Below is the overview of this notebook.\n",
    "\n",
    "0. Install required packages (only need to do this the first time we run the notebook)\n",
    "\n",
    "1. Load corpus of tweets\n",
    "\n",
    "2. Load sentiment classifier, which in this case is a BERT transformer\n",
    "\n",
    "3.  Measure tweet sentiment\n",
    "\n",
    "4. Analyze sentiment of tweets, compare to retweet counts\n",
    "\n",
    "5. Visualize UMAP transformer embeddings\n",
    "\n",
    "\n",
    "Below are some cool blogs you can read to learn more about the BERT transformer.\n",
    "\n",
    "http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77\n",
    "\n",
    "https://github.com/jessevig/bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DL2GzSr9olqL"
   },
   "source": [
    "## Install requirements\n",
    "\n",
    "The pip command lets you install python packages.  We need two packages today.\n",
    "\n",
    "1) transformers - this lets us use pre-trained transformer models. \n",
    "\n",
    "2) umap - this lets you embed high dimensional vectors in low dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-28iUDbvolqM",
    "outputId": "cc30972c-8e0c-4187-c40a-90998fd8bbd7"
   },
   "outputs": [],
   "source": [
    "#install transformers\n",
    "!pip install  transformers \n",
    "\n",
    "#install umap\n",
    "!pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyQj9OpuolqO"
   },
   "source": [
    "# Clone GitHub Repository\n",
    "This will clone the repository to your machine.  This includes the code and data files.  Then change into the directory of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3NG7yGoolqO",
    "outputId": "2b2f0323-018f-4e8e-ff51-32267afeff2d"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/zlisto/sentiment_analysis\n",
    "\n",
    "import os\n",
    "os.chdir(\"sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXtkVNR4olqP"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZqLAIBoolqP",
    "outputId": "d6fe048b-35c7-47c5-80cf-d4d937a9a3a4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import codecs\n",
    "import umap\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"device ={device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXW-xMvIolqQ"
   },
   "source": [
    "# Sentiment Classification with BERT\n",
    "\n",
    "We will pass the tweets through a pre-trained sentiment classifier with a BERT core.  Then we will plot the tweets with UMAP and color them by their sentiment.  Hopefully the positive and negative are in different regions of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L14UJuj-olqR"
   },
   "source": [
    "### Download Pre-Traine Model and Tokenizer\n",
    "\n",
    "We will download the model and tokenizer from https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment.  This is a pre-trained model in the huggingface library that was trained on product reviews in multiple languages.  The output sentiment is between 1 and 5.\n",
    "\n",
    "There are many other models on huggingface that you can find here: https://huggingface.co/models?pipeline_tag=text-classification.\n",
    "\n",
    "We will create a tokenizer for the model called `tokenizer` and create the model itself, which we call `model`.  Every model needs its own tokenizer which tells it how to map text into the proper input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HE3aCKl9olqR"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BavflE-dolqS"
   },
   "source": [
    "### Define Sentiment Classifier and Transformer Embedding Function\n",
    "\n",
    "When we pass text through our transformer model, we get many pieces of data in the output. First, we get the sentiment of the text.  Second, we get the text embedding in the final layer of the transformer.  Remember, inside the transformer we turn the input text into a high dimensional vector. This is the transformer embedding, and it is designed to separate text based on sentiment.\n",
    "\n",
    "\n",
    "We will create a function called `sentiment_classifier` which takes as input a string `text`, a transformer model called `model`, and a tokenizer called `tokenizer`, and returns the sentiment and embedding of the text.  The raw sentiment output of the model is a probability for each sentiment value.  The function will return the average sentiment based on these probabilities.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uepDC1i5olqS"
   },
   "outputs": [],
   "source": [
    "def sentiment_classifier(text,model,tokenizer):\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    output = model(input_ids, token_type_ids=token_type_ids,return_dict=True,output_hidden_states=True)\n",
    "    logits = np.array(output.logits.tolist()[0])\n",
    "    prob = np.exp(logits)/np.sum(np.exp(logits))\n",
    "    sentiment = np.sum([(x+1)*prob[x] for x in range(len(prob))])  #use this line if you want the mean score\n",
    "    embedding = output.hidden_states[12].detach().numpy().squeeze()[0]\n",
    "    \n",
    "    return sentiment, embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY_POtGtolqT"
   },
   "source": [
    "### Sentiment Classification Example\n",
    "Now we can test the model on some text.  Feel free to try any text you want here.  Just put your text in the list `Text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdKFFdAkolqT"
   },
   "outputs": [],
   "source": [
    "Text = [\"This class is kinda boring, but informative\", \n",
    "        \"This class is awesome\", \n",
    "        \"this class is dope\", \n",
    "        \"this class is stupid\",\n",
    "        \"this class is fun\",\n",
    "       \"this class is fun!\",\n",
    "       \"this class is :(\",\n",
    "       \"this class is :)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpOvF4sRolqU"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFebhr9oolqU",
    "outputId": "bdc79134-b818-4213-de3f-7f245e91cce4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in Text:\n",
    "    sentiment,embedding = sentiment_classifier(text,model,tokenizer)\n",
    "    print(f\"Text: {text}\\nSentiment:{sentiment:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxNwnT_polqV"
   },
   "source": [
    "### Load Tweets \n",
    "\n",
    "Load the tweets from the file `\"data/tweets_sentiment_embedding.csv\"` into a dataframe `df`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VHam01ik0nX7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyhHNkRiolqW",
    "outputId": "c4c9de02-c270-4944-e106-355d8a557d0e"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/tweets_sentiment_embedding.csv\")\n",
    "\n",
    "df = df[['screen_name','text','retweet_count']]\n",
    "ntweets = len(df)\n",
    "print(f\"dataframe has {ntweets} tweets\\n\")\n",
    "[print(f\"Tweets from {x}\") for x in df.screen_name.unique()];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1nagcNiolqW"
   },
   "source": [
    "### Calculate Sentiment and Transformer Embedding of Tweets\n",
    "\n",
    "We use a for loop iterating over the rows in `df` to calculate the sentiment and transformer embedding of each tweet.  We store the sentiment in the list `Sentiment` and embeddings in the list `Embedding`.  We then add the sentiment to a column in `df` and save the resulting dataframe to a file `\"data/lec_12_tweets_sentiment_embedding.csv\"`.  This may take a long time on your machine unless you have a GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AUyR6W7olqX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBaIJ4alolqX",
    "outputId": "18cbfbf3-7446-4b44-c643-482d05cc6d32"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c = 0\n",
    "Sentiment = []\n",
    "Embedding = []\n",
    "for index,row in df.iterrows():\n",
    "    c+=1\n",
    "    if c%1000==0:print(f\"Tweet {c}/{len(df_tweets)}\")\n",
    "    sentiment,embedding = sentiment_classifier(row.text,model,tokenizer)\n",
    "    Sentiment.append(sentiment)\n",
    "    Embedding.append(embedding)\n",
    "\n",
    "df['sentiment'] = Sentiment\n",
    "df.to_csv(\"data/tweets_sentiment.csv\")\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0xJ-e4lolqX"
   },
   "source": [
    "### UMAP Transformer Embedding of Tweets\n",
    "\n",
    "The tranformer turns the input text into a high dimensional vector.  This is the transformer embedding, and it is designed to separate text based on sentiment.  However, we can't really visualize such a high-dimensional object.  But no worries, UMAP will let us embed this high-dimensional vector into 2 dimensions.  UMAP stands for Uniform Manifold Approximation and Projection for Dimension Reduction.  You can read more about UMAP on its website https://umap-learn.readthedocs.io/en/latest/.\n",
    "\n",
    "We apply UMAP to the transformer embedding `Embedding` to create the UMAP transformer embedding `umap_bert_embedding`.  Before doing this we have to convert `Embedding` from a list to an array.  Then we save the UMAP embedding values in `df` as `\"umap_transformer_x\"` and `\"umap_transformer_y\"`.  The dataframe is saved to the file `\"data/lec_12_tweets_sentiment_embedding.csv\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwIkLnCo1djx"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d__lSFaTolqY",
    "outputId": "8dff9f75-c840-4510-8c51-9e86acede5ad"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "Embedding = np.array(Embedding)\n",
    "umap_bert_embedding = umap.UMAP(n_components = 2, metric='cosine').fit_transform(Embedding)\n",
    "df['umap_transformer_x'] = umap_bert_embedding[:,0]\n",
    "df['umap_transformer_y'] = umap_bert_embedding[:,1]\n",
    "\n",
    "df.to_csv('data/tweets_sentiment_embedding.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsfxwcBfolqY"
   },
   "source": [
    "# Analyze Tweet Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ALSW6yWolqY"
   },
   "source": [
    "### Load Tweets and Sentiment\n",
    "\n",
    "Once we save the tweet sentiments, the next time we run the notebook we can just load this data instead of recalculating the sentiment.  The sentiment is in the file `\"data/tweets_sentiment_embedding.csv\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "DjeFUKHpolqZ",
    "outputId": "92816a63-c9b7-43b1-e17a-3aa75b5cb2d0"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/tweets_sentiment_embedding.csv\")\n",
    "\n",
    "print(f\"dataframe has {len(df)} tweets\")\n",
    "[print(f\"Tweets from {x}\") for x in df.screen_name.unique()];\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA-JbCDKolqZ"
   },
   "source": [
    "### Average User Sentiment\n",
    "\n",
    "We make a bar plot of the average sentiment of each user.  We do this by grouping `df` by `\"screen_name\"` and then taking the mean of each group with the `mean` function.  We save this a new dataframe `df_plot` and then use the `barplot` function to make the bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qdVBJfkolqZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ureNPfGolqZ",
    "outputId": "67b1a193-3301-4781-f355-dc8298a1cf82"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,6))\n",
    "sns.barplot(data = df, y= 'sentiment', x = 'screen_name' )\n",
    "plt.ylim([2,4])\n",
    "plt.ylabel(\"Mean sentiment\",fontsize = 16)\n",
    "plt.grid()\n",
    "plt.ylim([0,5]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRCVIguZolqa"
   },
   "source": [
    "### Sentiment Distribution per User\n",
    "\n",
    "We can make histograms of the tweet sentiment for each user.  We use a for loop to iterate through each screen name.\n",
    "We use the `histplot` function to make a histogram of the tweets of each user.  We also add a title to each plot so we know whose tweets they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoGeM2Wuolqa"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqPphbUkolqa",
    "outputId": "9a16ab30-41ce-4cf0-f90b-9c83323b445d"
   },
   "outputs": [],
   "source": [
    "for screen_name in df.screen_name.unique():\n",
    "  sns.histplot(data = df[df.screen_name==screen_name], x = 'sentiment')\n",
    "  plt.title(f\"Tweets of {screen_name}\", fontsize = 14)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQQ4lSP-olqa"
   },
   "source": [
    "### Look at Tweets with Extreme Sentiment\n",
    "\n",
    "We can select tweets of each user with very high or very low sentiment and print them out.  We do this by keeping the rows of `df` with the corresponding screen name, and then using `sort_values` to sort the user's tweets by sentiment.  We set `ascending = True` inside the `for` loop to get the most postive tweets, and set `ascending = False` to get the most negative tweets.  We set `ndisplay` equal to the number of tweets we want to print per user.\n",
    "\n",
    "To show all the funny Twitter characters, we need to use the `decode` function in the `codecs` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p66qhqUDolqb",
    "outputId": "11459722-5f9f-4a28-8003-991496745c16"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DF8U3BPxolqb",
    "outputId": "2aafdfaf-5bd4-46f4-ec59-7f5506260752"
   },
   "outputs": [],
   "source": [
    "ndisplay = 3\n",
    "print(f\"Top {ndisplay} most postive tweets per screen name\")\n",
    "for screen_name in df.screen_name.unique():\n",
    "    df_display = df[(df.screen_name==screen_name)].sort_values(by = ['sentiment'], \n",
    "                                                               ascending = False)\n",
    "    c=0\n",
    "    print(f\"\\n{screen_name}\")\n",
    "    for index,row in df_display.iterrows():\n",
    "        c+=1\n",
    "        text = codecs.decode(row.text, 'unicode_escape')\n",
    "        print(f\"\\tsentiment = {row.sentiment:.2f}: {text}\")\n",
    "        if c>=ndisplay:break\n",
    "\n",
    "print(\"\".join('-' * 200))\n",
    "\n",
    "print(f\"Top {ndisplay} Most Negative Tweets per Screen Name\")\n",
    "for screen_name in df.screen_name.unique():\n",
    "    df_display = df[(df.screen_name==screen_name)].sort_values(by = ['sentiment'], ascending = True)\n",
    "    c=0\n",
    "    print(f\"\\n{screen_name}\")\n",
    "    for index,row in df_display.iterrows():\n",
    "        c+=1\n",
    "        text = codecs.decode(row.text, 'unicode_escape')\n",
    "        print(f\"\\tsentiment = {row.sentiment:.2f}: {text}\")\n",
    "        if c>=ndisplay:break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT3tzGfWolqb"
   },
   "source": [
    "### Box Plot of Retweet Count vs Sentiment\n",
    "\n",
    "To see this correlation of extreme sentiment and retweet count, we can make a box plot. The box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable. The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the inter-quartile range.  More details on the `boxplot` function can be found here: https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
    "\n",
    " We first create a column called `star` that rounds the sentiment to the nearest integer.  Then we can make a boxplot of the retweet count versus the tweet star sentiment.  We will have to limit the y-axis in order to see the pattern clearly.  We set the maximum value on the y-axis equal to `ymax`, which we set to be an upper quantile of the retweet count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "pfBAkLgdolqb",
    "outputId": "5e698f2b-69d5-4282-ecfd-81ff12acf29a"
   },
   "outputs": [],
   "source": [
    "df['star'] = df.sentiment.round()\n",
    "\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "sns.boxplot(data = df, x= \"star\", y = 'retweet_count')\n",
    "ymax = df.retweet_count.quantile(0.95)\n",
    "plt.ylim([0,1e4])\n",
    "plt.xlabel(\"Sentiment\", fontsize = 16)\n",
    "plt.ylabel(\"Retweet Count\", fontsize = 16)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wadO5liolqb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['star'] = df.sentiment.round()\n",
    "\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "sns.boxplot(data=df, x=\"star\", y=\"retweet_count\")\n",
    "ymax = df.retweet_count.quantile(0.95)\n",
    "plt.ylim([0,ymax])\n",
    "plt.xlabel(\"Sentiment\", fontsize = 16)\n",
    "plt.ylabel(\"Retweet Count\", fontsize = 16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U7Fms1dolqc"
   },
   "source": [
    "### Box Plot per User\n",
    "\n",
    "We can also look at a boxplot for each individual user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Kn_7Er1wolqc",
    "outputId": "5381b0f8-83c9-4659-bbe0-fd5adea65fde"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSU-03LHolqc",
    "outputId": "283ec979-0ed1-4146-d438-b3eaf367e961"
   },
   "outputs": [],
   "source": [
    "for screen_name in df.screen_name.unique():\n",
    "    fig = plt.figure(figsize = (12,8))\n",
    "    df_plot = df[df.screen_name==screen_name]\n",
    "    ax = sns.boxplot(data=df_plot, x=\"star\", y=\"retweet_count\")\n",
    "    ymax = df_plot.retweet_count.quantile(q=0.9)\n",
    "    plt.ylim([0,ymax])\n",
    "    plt.xlabel(\"Sentiment\", fontsize = 16)\n",
    "    plt.ylabel(\"Retweet Count\", fontsize = 16)\n",
    "    plt.title(f\"{screen_name}\", fontsize = 16)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9e80Btjolqc"
   },
   "source": [
    "# Visualize Transformer Embedding\n",
    "\n",
    "Now we will visulize the transformer embeddings using UMAP to see how the sentiment is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DENTU9j0olqc"
   },
   "source": [
    "### Scatter Plot of UMAP Transformer Tweet Embeddings\n",
    "\n",
    "We can make a scatter plot of the UMAP transformer embeddings of the tweets.  We will color the data points by the user screen name.  We will also make another plot next to this plot where we color the data points by sentiment.  You set the column for the datapoint color with the `hue` parameter.  You can choose a color palette with the `palette` parameter.  There are many palettes you can choose from, but for discrete values like `\"screen_name\"` use the `\"bright\"` palette and for continous values like `\"sentiment\"` use the `\"vlag\"` palette.  Of course feel free to try other palettes. A complete list can be found here: https://seaborn.pydata.org/tutorial/color_palettes.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0UZVvs0olqc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYPeEq_0olqd"
   },
   "outputs": [],
   "source": [
    "for screen_name in df.screen_name.unique():\n",
    "  fig = plt.figure(figsize = (8,8))\n",
    "  sns.scatterplot(data=df[df.screen_name == screen_name], \n",
    "                  x=\"umap_transformer_x\", \n",
    "                  y=\"umap_transformer_y\", \n",
    "                  hue=\"sentiment\", \n",
    "                  palette=\"vlag\", s=15)\n",
    "  plt.title(f\"UMAP Transformer Embedding for {screen_name}\")\n",
    "  plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
